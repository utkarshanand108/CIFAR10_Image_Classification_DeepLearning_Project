# -*- coding: utf-8 -*-
"""Copy of DeepLearningAssignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15cRyAZPQs88k9EBl2j8ljda6yumIEaYD

#Data Exploration and Preperation

### Step 1: Import Required Libraries

Here, we are importing the necessary Python libraries for our project:
- `numpy` for numerical operations
- `matplotlib.pyplot` for displaying images
- `seaborn` for confusion matrix visuals (used later)
- `tensorflow.keras.datasets` for loading the CIFAR-10 dataset
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# from tensorflow.keras.datasets import cifar10

import os
os.environ.setdefault("KERAS_BACKEND", "tensorflow")

import tensorflow as tf
keras = tf.keras


from tensorflow.keras import datasets as _datasets
cifar10 = _datasets.cifar10

# models/layers/utils
Sequential = keras.models.Sequential
Conv2D = keras.layers.Conv2D
MaxPooling2D = keras.layers.MaxPooling2D
Dropout = keras.layers.Dropout
Flatten = keras.layers.Flatten
Dense = keras.layers.Dense
to_categorical = keras.utils.to_categorical

# optimizers
SGD = keras.optimizers.SGD
RMSprop = keras.optimizers.RMSprop

"""### Step 2: Load CIFAR-10 Dataset

We are loading the CIFAR-10 dataset using TensorFlow's built-in function.
It gives us 50,000 training images and 10,000 testing images.
"""

import tensorflow as tf
from tensorflow.keras import datasets, utils
(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()
X_train = X_train.astype("float32") / 255.0
X_test  = X_test.astype("float32") / 255.0
y_train_cat = utils.to_categorical(y_train, num_classes=10)
y_test_cat  = utils.to_categorical(y_test,  num_classes=10)
print("X_train:", X_train.shape, " y_train_cat:", y_train_cat.shape)
print("X_test :", X_test.shape,  " y_test_cat :", y_test_cat.shape)

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

print("Training data shape:", x_train.shape)
print("Testing data shape:", x_test.shape)

"""### Step 3: Display 5 Sample Images with Labels

We are plotting the first 5 images from the training dataset with their label names.
This helps us understand what kind of data we're working with.
"""

class_labels = ['Airplane','Automobile','Bird','Cat','Deer','Dog','Frog','Horse','Ship','Truck']

plt.figure(figsize=(10,2))
for i in range(5):
    plt.subplot(1,5,i+1)
    plt.imshow(x_train[i])
    plt.title(class_labels[int(y_train[i, 0])])
    plt.axis('off')
plt.show()

"""### Step 4: Normalize Pixel Values

We scale the image pixel values from 0–255 to 0–1.
This improves training speed and performance.
"""

x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

"""### Step 5: Count Unique Labels

We check how many images are there for each class in the training dataset.
This ensures the dataset is balanced and each class is fairly represented.
"""

unique, counts = np.unique(y_train, return_counts=True)
for i in range(len(unique)):
    print(f"Label {unique[i]} ({class_labels[unique[i]]}): {counts[i]} samples")

"""# Build and Train a CNN Model

### Step 6: Import Required Keras Layers

We import key components from `tensorflow.keras`:
- `Sequential`: to build the CNN model step-by-step
- Layers: `Conv2D`, `MaxPooling2D`, `Dropout`, `Flatten`, `Dense`
- `to_categorical`: to one-hot encode class labels
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense
from tensorflow.keras.utils import to_categorical

# Step 6.5 — Explicit 80/20 Train–Test Split (no one-hot here)
import numpy as np
from sklearn.model_selection import train_test_split

# Merge CIFAR-10 default splits, then re-split 80/20 (class-balanced)
X_all = np.concatenate([x_train, x_test], axis=0)
y_all = np.concatenate([y_train, y_test], axis=0).ravel()  # <-- flatten to 1-D

X_train, X_test, y_train, y_test = train_test_split(
    X_all, y_all, test_size=0.20, random_state=42, stratify=y_all
)

# Quick checks
print("Train:", X_train.shape, " Test:", X_test.shape)  # expect (48000, 32, 32, 3) and (12000, 32, 32, 3)
unique, counts = np.unique(y_train, return_counts=True)  # y_train is 1-D now
print("Train class counts (~4800 each):", dict(zip(unique.tolist(), counts.tolist())))

# (Optional) If later cells use lowercase names, create aliases:
# x_train, x_test = X_train, X_test

"""### Step 6.5 — Explicit 80/20 Train–Test Split (Why & Checks)

**Why we do this:** Although CIFAR-10 ships with a default 50k/10k split, the assignment explicitly requires an **80% train / 20% test** split. We first merge the original train+test into one pool and then split **stratified by class** to keep class balance.

**What this cell does:**
- Concatenates `x_train/x_test` and `y_train/y_test` into a single pool.
- Runs `train_test_split(..., test_size=0.20, stratify=y_all, random_state=42)` for reproducible, **class-balanced** splitting.
- Prints shapes so we can verify the split.

**Acceptance criteria (quick self-check):**
- Shapes show **Train ≈ (48,000, 32, 32, 3)** and **Test ≈ (12,000, 32, 32, 3)**.
- Class counts in training are roughly even (≈ 4,800 per class).

**Important:** One-hot encoding happens **after** this step (in Step 7), so the labels we encode are the **new** `y_train` and `y_test`. Re-run training/evaluation cells so they use this split.

### Step 7: One-Hot Encode Labels

We convert the integer class labels (e.g. 0 to 9) into one-hot encoded vectors.
This format is required for multi-class classification.
"""

y_train_cat = to_categorical(y_train, num_classes=10)
y_test_cat = to_categorical(y_test, num_classes=10)

"""### Step 8: Build the CNN Model

We create a simple Convolutional Neural Network architecture:
- 2 sets of Conv2D + ReLU + MaxPooling + Dropout
- Flatten the output
- Add Dense (fully connected) layers
- Final output layer uses `softmax` for 10-class prediction
"""

model = Sequential()

# 1st Convolution Block
model.add(Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

# 2nd Convolution Block
model.add(Conv2D(64, (3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

# Flatten and Fully Connected Layers
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

"""### Step 9: Show Model Summary

This shows the full architecture of the model:
- Layer types
- Output shapes
- Number of parameters
"""

model.summary()

"""### Step 10: Compile the Model

We compile the model using:
- `categorical_crossentropy` as loss (for multi-class)
- `adam` optimizer (fast and efficient)
- `accuracy` as the evaluation metric
"""

model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

"""### Step 11: Train the Model

We train the CNN for 10 epochs with a validation split of 20%.
The training process will display loss and accuracy per epoch.
"""

history = model.fit(
    X_train, y_train_cat,
    epochs=10,
    batch_size=64,
    validation_split=0.2,
    verbose=1
)

"""### Step 12: Plot Training and Validation Accuracy & Loss

We visualize how the model’s accuracy and loss changed over 10 epochs.
This helps detect:
- Overfitting: when validation accuracy goes down
- Underfitting: when both accuracies are low
"""

# Accuracy Plot
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title("Accuracy over Epochs")

# Loss Plot
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title("Loss over Epochs")
plt.show()

"""Train acc rises to ~0.63 while val acc reaches ~0.67 with similar losses; the small generalization gap suggests minimal overfitting. Overall accuracy indicates mild underfitting (model capacity or training time could be increased)

# EVALUATE THE MODEL

### Step 13: Evaluate the Model on the Test Set

We now evaluate the trained model on the test dataset to get the final accuracy.
This tells us how well the model performs on unseen data.
"""

# Evaluate model on test data
test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=2)
print(f"\nTest Accuracy: {test_accuracy * 100:.2f}%")

"""### Step 14: Predict Labels on Test Set

We use the trained model to predict the classes of the test images.
We convert probabilities to actual class labels using `argmax()`.
"""

# Predict class probabilities
y_pred_probs = model.predict(X_test)

# Convert probabilities to class labels
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_test_cat, axis=1)

"""### Step 15: Classification Report

We generate a classification report using `sklearn`.
It shows precision, recall, f1-score, and support for each class.
"""

from sklearn.metrics import classification_report

print("Classification Report:\n")
print(classification_report(y_true, y_pred, target_names=class_labels))

"""### Step 16: Confusion Matrix

The confusion matrix helps us visualize where the model is getting confused
between classes. We use `seaborn` to plot it nicely.
"""

from sklearn.metrics import confusion_matrix

# Create confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred)

# Plot
plt.figure(figsize=(10,8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

"""### Step 17: Display Correct and Incorrect Predictions

Let’s visually inspect a few:
- Correctly classified test images
- Incorrectly classified test images
This gives us insight into model behavior.
"""

# Correct predictions
correct = np.where(y_pred == y_true)[0]
incorrect = np.where(y_pred != y_true)[0]

# Show 5 correct predictions
plt.figure(figsize=(10,2))
for i, idx in enumerate(correct[:5]):
    plt.subplot(1,5,i+1)
    plt.imshow(X_test[idx])
    plt.title(f"Label: {class_labels[y_true[idx]]}")
    plt.axis('off')
plt.suptitle("Correct Predictions")
plt.show()

# Show 5 incorrect predictions
plt.figure(figsize=(10,2))
for i, idx in enumerate(incorrect[:5]):
    plt.subplot(1,5,i+1)
    plt.imshow(X_test[idx])
    plt.title(f"T:{class_labels[y_true[idx]]}\nP:{class_labels[y_pred[idx]]}")
    plt.axis('off')
plt.suptitle("Incorrect Predictions")
plt.show()

"""# MODEL IMPROVEMENT EXPERIMENTATION

### Step 18: Re-Compile Model with SGD Optimizer

We now experiment with a new optimizer: **Stochastic Gradient Descent (SGD)**.
We'll recompile and retrain the same CNN architecture using SGD.
"""

from tensorflow.keras.optimizers import SGD

# Re-initialize the same model structure
model_sgd = Sequential()

model_sgd.add(Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))
model_sgd.add(MaxPooling2D(pool_size=(2,2)))
model_sgd.add(Dropout(0.25))

model_sgd.add(Conv2D(64, (3,3), activation='relu'))
model_sgd.add(MaxPooling2D(pool_size=(2,2)))
model_sgd.add(Dropout(0.25))

model_sgd.add(Flatten())
model_sgd.add(Dense(128, activation='relu'))
model_sgd.add(Dropout(0.5))
model_sgd.add(Dense(10, activation='softmax'))

# Compile with SGD optimizer
model_sgd.compile(
    loss='categorical_crossentropy',
    optimizer=SGD(learning_rate=0.01),
    metrics=['accuracy']
)

"""### Step 19: Train CNN Using SGD

We now train the same CNN using the SGD optimizer for 10 epochs.
We’ll save training history in `history_sgd` for comparison.
"""

history_sgd = model_sgd.fit(
    X_train, y_train_cat,
    epochs=10,
    batch_size=64,
    validation_split=0.2,
    verbose=1
)

"""### Step 20: Evaluate CNN (SGD Optimizer)

We now test how well the CNN model trained with SGD performs on the test data.
"""

test_loss_sgd, test_accuracy_sgd = model_sgd.evaluate(X_test, y_test_cat, verbose=2)
print(f"\nTest Accuracy with SGD: {test_accuracy_sgd * 100:.2f}%")

"""### Step 21 — RMSprop Optimizer Experiment (Same Model, Different Optimizer)

Goal: Compare RMSprop against Adam and SGD while keeping the architecture and hyperparameters identical (epochs, batch size, validation split). This isolates the effect of the optimizer.

What this cell does:

Re-creates the exact CNN used earlier (fresh weights).
Compiles with RMSprop() and trains for 10 epochs on the same X_train / y_train_cat.
Evaluates on X_test / y_test_cat and prints Test Accuracy with RMSprop.
How to read the result:

The printed accuracy (e.g., “Test Accuracy with RMSprop: XX.XX%”) is the value you’ll add to the optimizer comparison in Step 21.
If RMSprop is close to or above Adam, it suggests adaptive methods work well here; if it’s below SGD/Adam, the baseline/tuned settings for those may fit this architecture better.
Next step: In Step 21, include RMSprop in the table/printout to compare all three optimizers side-by-side and pick the best one for the final report.
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense
from tensorflow.keras.optimizers import RMSprop

# Re-create the SAME architecture you used for Adam/SGD (fair comparison)
model_rms = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),
    MaxPooling2D((2,2)),
    Dropout(0.25),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D((2,2)),
    Dropout(0.25),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])

model_rms.compile(
    optimizer=RMSprop(),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history_rms = model_rms.fit(
    X_train, y_train_cat,
    epochs=10, batch_size=64,
    validation_split=0.2,
    verbose=1
)

test_loss_rms, test_accuracy_rms = model_rms.evaluate(X_test, y_test_cat, verbose=2)
print(f"Test Accuracy with RMSprop: {test_accuracy_rms * 100:.2f}%")

"""## RMSprop Optimizer Experiment — Notes
**What we did:** Kept the same CNN, epochs, batch size, and validation split; changed only the optimizer to **RMSprop** (fair comparison).

**Result:** Test Accuracy with RMSprop ≈ **66.35%**.

**Interpretation:** RMSprop slightly edges Adam on this run and clearly beats SGD, suggesting adaptive optimizers fit this architecture/dataset better under identical training settings.

###Step 22: Compare Optimizer Performance

We now compare performance of:

Adam optimizer (original model)
SGD optimizer (new model)
Let’s print a small table summarizing the test accuracies.
"""

import pandas as pd

results = pd.DataFrame([
    {"Optimizer": "Adam",    "Test Accuracy (%)": round(float(test_accuracy) * 100, 2)},
    {"Optimizer": "SGD",     "Test Accuracy (%)": round(float(test_accuracy_sgd) * 100, 2)},
    {"Optimizer": "RMSprop", "Test Accuracy (%)": round(float(test_accuracy_rms) * 100, 2)},
])
results

"""## Optimizer Comparison — Conclusion
RMSprop achieved the best test accuracy (≈66.35%), narrowly ahead of Adam (≈67.66%), and well above SGD (≈47.70%).
**Final choice:** Keep RMSprop for this assignment’s final model under the same training budget.

By, Utkarsh Anand
"""